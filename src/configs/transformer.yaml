project: "faab_autoencoder_transformer"
# dataset parameters
pickle_path: "data/0117/0117_processed_1024.pkl"
pred: False
seq_len: 1024
comp_seq_len: 1024
batch_size: 32

# common model parameters
feat_len: 8 # feature size
comp_feat_len: 4
ff_size: 12
num_layers: 4

# model parameters transformer
model: "transformer"
num_heads: 2
pe_scale_factor: 1
mask: False

# training parameters
max_grad_norm: 0
dropout: 0
epochs: 150
optimizer: "rmsprop"
criterion:
learning_rate: 0.0001
scheduler_step_size: 0
scheduler_gamma: 0.1
plotter_samples: 5
