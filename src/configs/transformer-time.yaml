project: "faab_autoencoder_transformer_time"
# dataset parameters
pickle_path: "07041536_processed_1024_overlap.pkl"
pred: False
seq_len: 1024
batch_size: 64

# common model parameters
feat_len: 8 # feature size
comp_feat_len: 4
ff_size: 12
num_layers: 4

# model parameters transformer
model: "transformer"
num_heads: 2
pe_scale_factor: 1
mask: False
comp_seq_len: 64

# training parameters
max_grad_norm: 0
dropout: 0
epochs: 500
optimizer: "rmsprop"
learning_rate: 0.0001
scheduler_step_size: 0
scheduler_gamma: 0.1
plotter_samples: 5
